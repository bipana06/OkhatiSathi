program: goat_finetune.py # Or your converted .py script name
method: bayes # Or random, grid
metric:
  name: val/loss # Metric to optimize (MUST match the name logged in train.py)
  goal: minimize
parameters:
  # --- Optimization ---
  learning_rate:
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-3
  batch_size:
    values: [16, 32] # Adjust based on GPU memory

  # --- Regularization ---
  weight_decay:
    distribution: log_uniform_values
    min: 1e-7
    max: 1e-5
  p_attention_dropout:
    values: [0.05, 0.1, 0.15]
  p_decoder_dropout:
    values: [0.05, 0.1, 0.15]

  # --- Early Stopping ---
  # Optional: Tune patience if desired
  early_stopping_patience:
    values: [8, 10, 15]

  # --- Other potential tunable params ---
  # grad_clip_thresh:
  #   values: [1.0, 5.0]
