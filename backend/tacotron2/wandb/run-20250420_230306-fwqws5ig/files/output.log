/scratch/md5121/okhatisathi/tacotron2/stft.py:67: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error
  fft_window = pad_center(fft_window, filter_length)
/scratch/md5121/okhatisathi/tacotron2/layers.py:51: FutureWarning: Pass sr=22050, n_fft=1024, n_mels=80, fmin=0.0, fmax=8000.0 as keyword args. From version 0.10 passing these as positional arguments will result in an error
  sampling_rate, filter_length, n_mel_channels, mel_fmin, mel_fmax)
--- Starting W&B Run: devoted-sweep-29 (ID: fwqws5ig) ---
Sweep Configuration for this run:
  Overriding hparams.batch_size: 32 -> 16
  Overriding hparams.early_stopping_patience: 10 -> 15
  Overriding hparams.learning_rate: 0.0001 -> 1.1335927322383887e-05
  Overriding hparams.p_attention_dropout: 0.1 -> 0.15
  Overriding hparams.p_decoder_dropout: 0.1 -> 0.15
  Overriding hparams.weight_decay: 1e-06 -> 7.658209696809138e-07
Run-specific Output Directory: ./outdir_sweep/fwqws5ig
--- Final HParams for Training Run ---
  epochs: 200
  seed: 1234
  distributed_run: False
  n_gpus: 1
  rank: 0
  group_name: group_name
  cudnn_enabled: True
  cudnn_benchmark: False
  fp16_run: False
  load_mel_from_disk: False
  training_files: ./filelists/train_list.txt
  validation_files: ./filelists/val_list.txt
  text_cleaners: ['transliteration_cleaners']
  num_workers: 4
  pin_memory: True
  max_wav_value: 32768.0
  sampling_rate: 22050
  filter_length: 1024
  hop_length: 256
  win_length: 1024
  n_mel_channels: 80
  mel_fmin: 0.0
  mel_fmax: 8000.0
  n_symbols: 148
  symbols_embedding_dim: 512
  encoder_kernel_size: 5
  encoder_n_convolutions: 3
  encoder_embedding_dim: 512
  n_frames_per_step: 1
  decoder_rnn_dim: 1024
  prenet_dim: 256
  max_decoder_steps: 1000
  gate_threshold: 0.5
  p_attention_dropout: 0.15
  p_decoder_dropout: 0.15
  attention_rnn_dim: 1024
  attention_dim: 128
  attention_location_n_filters: 32
  attention_location_kernel_size: 31
  postnet_embedding_dim: 512
  postnet_kernel_size: 5
  postnet_n_convolutions: 5
  use_saved_learning_rate: False
  learning_rate: 1.1335927322383887e-05
  weight_decay: 7.658209696809138e-07
  grad_clip_thresh: 1.0
  batch_size: 16
  mask_padding: True
  log_interval: 100
  iters_per_checkpoint: 1000
  early_stopping_patience: 15
  min_val_loss_delta: 0.0001
  ignore_layers: ['embedding.weight']
--------------------------------------
Calling train function...
TensorBoard Log directory: ./outdir_sweep/fwqws5ig/logs
Warm starting model from checkpoint 'tacotron2_statedict.pt'
  Applying warm start: Ignored 1 keys based on ignore_layers. Loading 83 keys.
Warm start complete. Optimizer and iteration count reset.
Starting training for 200 epochs...
--- Epoch: 0 ---
Train Iter: 0 Epoch: 0 [0/2367 (0%)]	Loss: 1.567846	Grad Norm: 13.113515	LR: 1.1E-05	Time: 6.19s/it
Train Iter: 100 Epoch: 0 [1600/2367 (68%)]	Loss: 0.751560	Grad Norm: 1.526752	LR: 1.1E-05	Time: 4.29s/it
--- Epoch 0 Summary ---
Average Training Loss: 1.012204
Average Grad Norm: 4.181522
Epoch Duration: 608.98s
--- Running Validation at Iteration 147 ---